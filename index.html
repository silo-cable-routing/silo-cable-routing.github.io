<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="SILO: Simulation-in-the-Loop Sim-to-Real Transfer for Multi-Stage Cable Routing"
    />
    <meta
      name="keywords"
      content="Robotics, Cable Routing, Reinforcement Learning, Simulation"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      SILO: Simulation-in-the-Loop Sim-to-Real Transfer for Multi-Stage Cable Routing
    </title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap"
      rel="stylesheet"
    />
    <link href="./public/index.css" rel="stylesheet" />
    <link href="./public/media.css" rel="stylesheet" />
    <link href="./public/sidebars.css" rel="stylesheet" />
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    <script src="./public/js/base.js"></script>
  </head>

  <body>
    <div class="sidebarsWrapper">
      <div class="sidebars">
        <a class="barWrapper" clear href="#abstract-a" id="bar2"
          ><span>Abstract</span>
          <div class="bar"></div
        ></a>
        <!-- <a class="barWrapper" clear href="#methodology-a" id="bar3"
          ><span>Methodology</span>
          <div class="bar"></div
        ></a> -->
        <a class="barWrapper" clear href="#results-a" id="bar4"
          ><span>Results</span>
          <div class="bar"></div
        ></a>
        
      </div>
    </div>
    <main class="content">
      <section class="heading">
        <h1 class="title">
          <span blue>SILO: </span><span blue>S</span>imulation-<span blue>I</span>n-the-<span blue>Lo</span>op Sim-to-Real Transfer for Multi-Stage Cable Routing
        </h1>
        <!-- <h3>conf name</h3> -->
        <section class="authors">
          <ul>
            <li>Anonymous</li>
          </ul>
        </section>
        <section class="affiliations">
          <ul>
          </ul>
        </section>
        <section class="links">
          <ul>
            <a
              href="https://openreview.net/forum?id=5kpVSo58BJ"
              rel="noreferrer"
              target="_blank"
            >
              <li>
                <span class="icon"> <img src="./public/paper.svg" /> </span
                ><span>Open Review</span>
              </li>
            </a>
          </ul>
        </section>
        <hr />
        <section>
          <div class="media">
            <img src="./public/figures/hero_figure.png" />
          </div>
          <br />
          <div class="media">
            <iframe width="100%" height="480" sandbox="allow-scripts allow-popups allow-forms allow-same-origin allow-popups-to-escape-sandbox allow-downloads allow-modals allow-storage-access-by-user-activation" src="https://www.youtube.com/embed/SAi7xzovn5o?si=nODDEvzdhRZ8aIzo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
          </div>
        </section>
        <a class="anchor" id="abstract-a"></a>
        <h2>Abstract</h2>
        <p class="abstract">
          Linear-deformable manipulation remains challenging due to the highly variable and difficult-to-predict deformations of objects such as cables and ropes. Prior data-driven approaches, particularly imitation learning, have shown some promise in narrowly defined settings but typically require thousands of demonstrations for a specific tasks and cable types, limiting scalability and generalization. We introduce a sim-to-real reinforcement learning (RL) framework for multi-stage cable routing that leverages a large-scale, GPU-parallelized simulation environment to efficiently approximate linear deformable behaviors. Training across thousands of parallel simulation environments enables the learned policies to generalize across diverse cable geometries and deformation patterns. To bridge the sim-to-real gap, we propose a novel deployment strategy that combines localized RL policies, a Simulation In the Loop (SILO) execution framework, and a robust state estimation pipeline. On real-world cable routing tasks, our approach achieves higher success rates and 2x reduction in cycle times than prior methods. To our knowledge, this is the first ever successful sim-to-real transfer of an RL policy for multi-stage cable routing tasks.
        </p>
      </section>
      <a class="anchor" id="methodology-a"></a>
      <section class="details">
        
      </section>
      <a class="anchor" id="results-a"></a>
      <section class="details">
        <h2>Results and Visualizations</h2>
        <p>Acompanying the paper, we provide video visualizations of various aspects of our system.</p>
        <h3>Sample Rollouts</h3>
        <p>Some example rollouts at 5x speed of our system. Our system can handle different configurations as well as different cable types.</p>
        <div class="sample-videos">
          <video autoplay="" muted="" loop="" controls>
            <source src="./public/videos/ethernet_cable_5x.mp4" type="video/mp4" />
          </video>
          <video autoplay="" muted="" loop="" controls>
            <source src="./public/videos/hdmi_cable_5x.mp4" type="video/mp4" />
          </video>
          <video autoplay="" muted="" loop="" controls>
            <source src="./public/videos/nylon_rope_5x.mp4" type="video/mp4" />
          </video>
          <video autoplay="" muted="" loop="" controls>
            <source src="./public/videos/tube_5x.mp4" type="video/mp4" />
          </video>
        </div>
        <h3>State Estimation Visualization</h3>
        <p>Video below shows how every prediction the state estimation system makes as our system routes a cable through multiple harnesses. Left video shows the simulation digital twin that is synchronized with the real world, with the cable visualized as a sequence of colored points. The right side shows an overlay of the real image, simulation image, and the predicted cable segmentation mask.</p>
        <div class="media">
          <video autoplay="" muted="" loop="" controls>
            <source src="./public/videos/state_estimation_visualization.mp4" type="video/mp4" />
          </video>
        </div>
        <h3>Learned behaviors</h3>
        <p>Videos below show some of the emergent behaviors we observed our RL policy learning. None of these behaviors are explicitly programmed or rewarded, but are learned due to the simple nature of the reward function.</p>
        <h4>Swinging behavior</h4>
        <p>The two videos below show the emergent swinging behavior learned by the RL policy. The robot executes lateral movements in response to the curvature of the cable relative to the harness. Notably, this behavior is not explicitly encoded in the reward function and emerges from interaction with the simulated dynamics during training.</p>
        <div class="learned-policies">
          <div style="display: flex; justify-content: space-between;flex-wrap: wrap;">
            <div class="media" style="width: 50%;">
              <video autoplay="" muted="" loop="" controls>
                <source src="./public/videos/swinging_left.mp4" type="video/mp4" />
              </video>
            </div>
            <div class="media" style="width: 50%;">
              <video autoplay="" muted="" loop="" controls>
                <source src="./public/videos/swinging_right.mp4" type="video/mp4" />
              </video>
            </div>
            <p class="caption" style="width: 50%;">When the cable is curved to the right of the harness, the policy swings the gripper leftward around the fixture.</p>
            <p class="caption" style="width: 50%;">When the cable is curved to the left of the harness, the policy swings the gripper rightward around the fixture.</p>
          </div>
          <h4>Angling behavior</h4>
          <p>The video below is an overlay of two videos. The normal-color (white rope and gripper) overlay shows the RL policy changing the gripper orientation as it routes the cable.
            The blue-tinted overlay shows a scripted baseline that follows the same translational trajectory as the RL policy but maintains a fixed gripper orientation. The video demonstrates how the RL policy's learned behavior depresses the cable further down towards the harness, which helps lead to more successes and faster routes.</p>
          <div class="learned-policies">
            <div class="media" style="width: 100%;">
              <video autoplay="" muted="" loop="" controls>
                <source src="./public/videos/angling_behavior.mp4" type="video/mp4" />
              </video>
            </div>
          </div>
        </div>
      </section>
      <!-- <section>
      </section> -->
      <!-- <section class="citation">
        <h2>Citation</h2>
        <pre><code></code></pre>
      </section> -->
      <!-- <section class="acknowledgements">
        <h2>Acknowledgements</h2>
        <p>
          TODO
        </p>
      </section> -->
    </main>
  </body>
</html>
